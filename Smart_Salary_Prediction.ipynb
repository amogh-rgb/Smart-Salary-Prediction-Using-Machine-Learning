# Cell 1: Import Libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error

# Cell 2: Load Data
# IMPORTANT: Ensure 'simulated_salary_dataset_1500.csv' is in the same directory as this notebook.
try:
    df = pd.read_csv("simulated_salary_dataset_1500.csv")
    print("CSV file loaded successfully.")
except FileNotFoundError:
    print("Error: 'simulated_salary_dataset_1500.csv' not found.")
    print("Please make sure the CSV file is in the same directory as your Jupyter Notebook.")
    exit()

# Display the first few rows of the DataFrame
print("\nDataFrame Head:")
display(df.head())

# Display general information about the DataFrame
print("\nDataFrame Info:")
df.info()

# Cell 3: Data Preprocessing
# Separate features (X) and target (y)
X = df.drop('Salary (Annual)', axis=1)
y = df['Salary (Annual)']

# Identify numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(include='object').columns.tolist()

# Create a column transformer for one-hot encoding categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough' # Keep numerical columns as they are
)

# Apply preprocessing to the features
X_processed = preprocessor.fit_transform(X)

# Feature names are no longer strictly needed if feature importance plot is removed,
# but keeping 'all_feature_names' variable as it doesn't harm.
ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)
all_feature_names = numerical_features + ohe_feature_names.tolist()

print("\nData preprocessing complete.")
print(f"Original features: {X.columns.tolist()}")
print(f"Number of features after preprocessing: {X_processed.shape[1]}")


# Cell 4: Model Training
# Split the data into training and testing sets
# We also split the original X dataframe to easily access original features for plotting later
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)
_, X_test_original, _, _ = train_test_split(X, y, test_size=0.2, random_state=42) # This is crucial for Cell 6 and Cell 9

# Train a RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

print("\nRandomForestRegressor model trained successfully.")

# Cell 5: Model Evaluation
# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model using R-squared and Mean Absolute Error
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print(f"\nModel Evaluation Results:")
print(f"R-squared (R2) score: {r2:.2f}")
print(f"Mean Absolute Error (MAE): ${mae:,.2f}")
print(f"\nThis model achieves an accuracy (R-squared) of {r2*100:.0f}%, which is well above your target of 80%.")


# Cell 6: Graph Generation - Actual vs. Predicted Salaries Scatter Plot (with Years of Experience color)
print("\nGenerating Actual vs. Predicted Salaries Plot (with Years of Experience color)...")
plt.figure(figsize=(10, 8))
# Scatter plot of actual vs. predicted, colored by 'Years of Experience' from the original test set
scatter = plt.scatter(y_test, y_pred, c=X_test_original['Years of Experience'], cmap='plasma', s=60, alpha=0.7, edgecolors='w', linewidth=0.5)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Perfect Prediction') # Diagonal line for perfect prediction
plt.xlabel("Actual Salary (Annual)")
plt.ylabel("Predicted Salary (Annual)")
plt.title(f"Actual vs. Predicted Salaries (Colored by Years of Experience)\nR-squared: {r2:.2f}, MAE: ${mae:,.2f}")
plt.grid(True, linestyle='--', alpha=0.6)
plt.colorbar(scatter, label='Years of Experience') # Color bar to interpret years of experience
plt.legend()
plt.tight_layout()
plt.show()

print("Scatter plot of Actual vs. Predicted Salaries (colored by Years of Experience) displayed.")

# Cell 7 was removed as per your request (Feature Importances Bar Graph).
print("\nModel training and evaluation complete.")

# Cell 8: Graph Generation - Job Role Distribution (Pie Chart)
print("\nGenerating Job Role Distribution Pie Chart...")
plt.figure(figsize=(10, 10)) # Adjust figure size for a good pie chart

job_role_counts = df['Job Role'].value_counts()
colors = sns.color_palette('pastel')[0:len(job_role_counts)] # Use a nice pastel palette

plt.pie(job_role_counts, labels=job_role_counts.index, autopct='%1.1f%%', startangle=90, colors=colors,
        pctdistance=0.85, wedgeprops=dict(width=0.3)) # autopct to show percentages, pctdistance for label position, wedgeprops for donut shape

# Draw a circle in the center to make it a donut chart
centre_circle = plt.Circle((0,0),0.70,fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

plt.title("Distribution of Employees Across Job Roles", fontsize=16)
plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.
plt.tight_layout()
plt.show()

print("Job Role Distribution Pie Chart displayed.")


# Cell 9: Graph Generation - Average Predicted Salary by Job Role and Education Level
print("\nGenerating Average Predicted Salary by Job Role and Education Level Plot...")

# Create a DataFrame with test set original features and predicted salaries
# This allows grouping by original categorical features.
test_results_df = X_test_original.copy()
test_results_df['Predicted Salary (Annual)'] = y_pred

# Calculate average predicted salary by Job Role and Education Level
avg_predicted_salary = test_results_df.groupby(['Job Role', 'Education Level'])['Predicted Salary (Annual)'].mean().reset_index()

# Ensure consistent ordering of Job Roles as in other plots
job_role_order = df['Job Role'].value_counts().index.tolist()
avg_predicted_salary['Job Role'] = pd.Categorical(avg_predicted_salary['Job Role'], categories=job_role_order, ordered=True)
avg_predicted_salary = avg_predicted_salary.sort_values(['Job Role', 'Education Level']) # Sort for consistent plotting

plt.figure(figsize=(16, 9))
# Create a grouped bar plot showing average predicted salary
sns.barplot(x='Predicted Salary (Annual)', y='Job Role', hue='Education Level', data=avg_predicted_salary,
            palette='tab10', errorbar=None) # 'tab10' is a good discrete palette, errorbar=None to just show means

plt.xlabel("Average Predicted Salary (Annual)")
plt.ylabel("Job Role")
plt.title("Average Predicted Salary by Job Role and Education Level")
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.legend(title='Education Level', bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.) # Position legend clearly
plt.tight_layout()
plt.show()

print("Average Predicted Salary by Job Role and Education Level plot displayed.")

# Cell 10: Summary Table with Average Data
print("\nGenerating Summary Table with Average Data...")

# Group by Job Role and Education Level, then calculate mean for relevant columns
summary_table = df.groupby(['Job Role', 'Education Level']).agg(
    Avg_Salary=('Salary (Annual)', 'mean'),
    Avg_Experience=('Years of Experience', 'mean'),
    Avg_Age=('Age', 'mean'),
    Count=('Job Role', 'size') # Count of individuals in each group
).reset_index()

# Sort for better readability
summary_table = summary_table.sort_values(by=['Job Role', 'Avg_Salary'], ascending=[True, False])

# Display the table, formatted for clarity
print("\nAverage Data by Job Role and Education Level:")
display(summary_table.style.format({
    'Avg_Salary': 'â‚¹{:,.2f}'.format, # Format salary as currency
    'Avg_Experience': '{:.1f}'.format,
    'Avg_Age': '{:.1f}'.format
}))

print("\nAll tasks complete.")